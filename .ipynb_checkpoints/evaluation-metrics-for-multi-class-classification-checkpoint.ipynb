{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.030258,
     "end_time": "2021-03-22T16:59:25.542707",
     "exception": false,
     "start_time": "2021-03-22T16:59:25.512449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Introduction</h1><a id = \"1\" ></a>\n",
    "\n",
    "When it comes to machine learning problems, there are lot of different types of metrics in the real world. We will see some of the most common metrics that we can use when starting with our projects.\n",
    "\n",
    "If we talk about classification problems, the most common metrics used are:\n",
    "- Accuracy\n",
    "- Precision (P)\n",
    "- Recall (R)\n",
    "- F1 score (F1)\n",
    "- Area under the ROC (Receiver Operating Characteristic) curve or simply AUC (AUC)\n",
    "\n",
    "![](https://blog.skyl.ai/hs-fs/hubfs/Evaluating%20a%20Machine%20Learning%20Model-min.jpg?width=1425&name=Evaluating%20a%20Machine%20Learning%20Model-min.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031102,
     "end_time": "2021-03-22T16:59:25.605133",
     "exception": false,
     "start_time": "2021-03-22T16:59:25.574031",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**In this notebook my focus will be on Multiclass classification problems means how we can use above mentioned evaluation metrics for Multiclass classification problems**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:25.675200Z",
     "iopub.status.busy": "2021-03-22T16:59:25.674439Z",
     "iopub.status.idle": "2021-03-22T16:59:27.263938Z",
     "shell.execute_reply": "2021-03-22T16:59:27.263062Z"
    },
    "papermill": {
     "duration": 1.628352,
     "end_time": "2021-03-22T16:59:27.264171",
     "exception": false,
     "start_time": "2021-03-22T16:59:25.635819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:27.334610Z",
     "iopub.status.busy": "2021-03-22T16:59:27.333787Z",
     "iopub.status.idle": "2021-03-22T16:59:27.391396Z",
     "shell.execute_reply": "2021-03-22T16:59:27.390748Z"
    },
    "papermill": {
     "duration": 0.094249,
     "end_time": "2021-03-22T16:59:27.391556",
     "exception": false,
     "start_time": "2021-03-22T16:59:27.297307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine = pd.read_csv('winequality-red.csv')\n",
    "df_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:27.473994Z",
     "iopub.status.busy": "2021-03-22T16:59:27.466442Z",
     "iopub.status.idle": "2021-03-22T16:59:27.478820Z",
     "shell.execute_reply": "2021-03-22T16:59:27.478259Z"
    },
    "papermill": {
     "duration": 0.055285,
     "end_time": "2021-03-22T16:59:27.478963",
     "exception": false,
     "start_time": "2021-03-22T16:59:27.423678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "df_wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:27.551752Z",
     "iopub.status.busy": "2021-03-22T16:59:27.551018Z",
     "iopub.status.idle": "2021-03-22T16:59:27.557163Z",
     "shell.execute_reply": "2021-03-22T16:59:27.556482Z"
    },
    "papermill": {
     "duration": 0.04566,
     "end_time": "2021-03-22T16:59:27.557309",
     "exception": false,
     "start_time": "2021-03-22T16:59:27.511649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df_wine.drop('quality', axis = 1)\n",
    "y = df_wine['quality']\n",
    "\n",
    "####ADD YOUR CODE HERE####\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 893)\n",
    "####END CODE HERE####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032416,
     "end_time": "2021-03-22T16:59:27.622428",
     "exception": false,
     "start_time": "2021-03-22T16:59:27.590012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's create a common trained model which we can use to illustrate various evaluation metrics\n",
    "\n",
    "Read about Random Forest Classifier in [this link](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:27.695102Z",
     "iopub.status.busy": "2021-03-22T16:59:27.694400Z",
     "iopub.status.idle": "2021-03-22T16:59:28.146106Z",
     "shell.execute_reply": "2021-03-22T16:59:28.145517Z"
    },
    "papermill": {
     "duration": 0.491383,
     "end_time": "2021-03-22T16:59:28.146260",
     "exception": false,
     "start_time": "2021-03-22T16:59:27.654877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred= [7 5 6 6 6 6 5 5 5 5 5 6 6 6 6 6 5 5 5 6 5 5 5 6 6 6 7 6 6 6 6 5 6 7 5 5 5\n",
      " 6 5 5 5 5 5 7 6 5 5 5 5 6 5 6 5 5 5 6 6 6 6 6 5 5 6 5 7 6 5 5 5 5 5 6 5 5\n",
      " 5 7 5 6 5 5 6 5 6 6 7 7 5 5 6 6 6 5 6 6 6 6 5 6 5 6 6 6 6 5 6 5 5 7 6 6 6\n",
      " 5 7 5 5 5 6 6 6 5 6 5 6 6 5 6 6 6 6 5 7 5 5 7 5 5 6 7 6 5 7 5 6 6 5 5 6 5\n",
      " 5 6 5 5 5 6 6 8 5 6 7 5 5 5 6 7 7 6 6 6 5 5 7 6 5 5 6 7 5 5 5 5 6 6 6 6 6\n",
      " 5 5 6 7 5 5 6 7 5 7 6 5 5 6 6 6 6 5 5 5 7 6 5 5 5 6 6 5 5 6 6 6 5 5 5 5 6\n",
      " 6 6 5 6 6 6 5 6 6 5 6 5 6 6 5 6 5 6 5 5 6 5 6 6 6 7 7 7 5 5 5 5 5 5 5 6 7\n",
      " 6 5 5 5 5 5 6 5 6 5 6 5 6 5 6 7 7 6 7 5 6 5 6 6 6 6 5 7 6 5 6 5 6 5 7 6 5\n",
      " 5 5 5 7 6 6 6 5 5 6 5 5 6 6 6 5 5 6 7 7 5 7 6 6]\n"
     ]
    }
   ],
   "source": [
    "#Common Trained Model\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "###ADD YOUR CODE HERE###\n",
    "\"\"\"\n",
    "You have two tasks here:\n",
    "1. Train the model using the training data. Use the fit method by looking at the documentation.\n",
    "2. Predict the quality of the wine using the test data. Use the predict method by looking at the documentation.\n",
    "\"\"\"\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "###END CODE HERE###\n",
    "print(\"y_pred=\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032418,
     "end_time": "2021-03-22T16:59:28.212385",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.179967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Accuracy metrics</h1><a id = \"2\" ></a>\n",
    "\n",
    "- **[Accuracy](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : It is one of the most straightforward metrics used in machine learning. It defines how accurate your model is. For example, if you build a model that classifies 90 samples accurately, your accuracy is 90% or 0.90. If only 83 samples are classified correctly, the accuracy of your model is 83% or 0.83. Simple.\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:28.284234Z",
     "iopub.status.busy": "2021-03-22T16:59:28.283589Z",
     "iopub.status.idle": "2021-03-22T16:59:28.286861Z",
     "shell.execute_reply": "2021-03-22T16:59:28.286304Z"
    },
    "papermill": {
     "duration": 0.041912,
     "end_time": "2021-03-22T16:59:28.287012",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.245100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate accuracy\n",
    "    -> param y_true: list of true values\n",
    "    -> param y_pred: list of predicted values\n",
    "    -> return: accuracy score\n",
    "    \n",
    "    \"\"\"\n",
    "    #### ADD YOUR CODE HERE ####\n",
    "    # Intitializing variable to store count of correctly predicted classes\n",
    "    actual_y = list(y_true)\n",
    "    correct_predictions = 0;\n",
    "    for i in range(len(actual_y)):\n",
    "        if actual_y[i] == y_pred[i]:\n",
    "            correct_predictions +=1\n",
    "    #returns accuracy\n",
    "    return correct_predictions / len(y_true)\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:28.355653Z",
     "iopub.status.busy": "2021-03-22T16:59:28.355011Z",
     "iopub.status.idle": "2021-03-22T16:59:28.361198Z",
     "shell.execute_reply": "2021-03-22T16:59:28.361682Z"
    },
    "papermill": {
     "duration": 0.042039,
     "end_time": "2021-03-22T16:59:28.361869",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.319830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032582,
     "end_time": "2021-03-22T16:59:28.427736",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.395154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can also calculate accuracy using scikit-learn.\n",
    "\n",
    "**[Scikit-learn user guide for accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:28.510633Z",
     "iopub.status.busy": "2021-03-22T16:59:28.509704Z",
     "iopub.status.idle": "2021-03-22T16:59:28.513137Z",
     "shell.execute_reply": "2021-03-22T16:59:28.513888Z"
    },
    "papermill": {
     "duration": 0.050697,
     "end_time": "2021-03-22T16:59:28.514117",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.463420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65625\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041074,
     "end_time": "2021-03-22T16:59:28.595377",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.554303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Precision</h1><a id = \"3\" ></a>\n",
    "\n",
    "\n",
    "- **[Precision](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** :  Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good\n",
    "\n",
    "     **[True Positives (TP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted  class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n",
    "\n",
    "     **[True Negatives (TN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n",
    "\n",
    "     False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n",
    "\n",
    "     **[False Positives (FP)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** – When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n",
    "\n",
    "     **[False Negatives (FN)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** – When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n",
    "     \n",
    "\n",
    "<h2 style = \"text-align: center\"> Precision = TP / (TP + FP) </h2>\n",
    "\n",
    "If we have to define accuracy using the terms described above, we can write:\n",
    "\n",
    "<h2 style = \"text-align: center\">Accuracy Score = (TP + TN) / (TP + TN + FP + FN) </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032814,
     "end_time": "2021-03-22T16:59:28.666211",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.633397",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's Create functions to compute True Positives, True Negatives, False Positives and False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:28.755781Z",
     "iopub.status.busy": "2021-03-22T16:59:28.752028Z",
     "iopub.status.idle": "2021-03-22T16:59:28.759843Z",
     "shell.execute_reply": "2021-03-22T16:59:28.759303Z"
    },
    "papermill": {
     "duration": 0.056345,
     "end_time": "2021-03-22T16:59:28.759996",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.703651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to compute True Positives, True Negatives, False Positives and False Negatives\n",
    "\n",
    "def true_positive(y_true, y_pred):\n",
    "    \n",
    "    tp = 0\n",
    "    \n",
    "    ### ADD YOUR CODE HERE ###\n",
    "    # Compute the true positives here\n",
    "    actual_y = list(y_true)\n",
    "    for i in range(len(y_pred)):\n",
    "        if actual_y[i] == 1 and y_pred[i] == 1:\n",
    "            tp +=1\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \n",
    "    tn = 0\n",
    "    \n",
    "    ### ADD YOUR CODE HERE ###\n",
    "    # Compute the true negatives here\n",
    "    actual_y = list(y_true)\n",
    "    for i in range(len(y_pred)):\n",
    "        if actual_y[i] == 0 and y_pred[i] == 0:\n",
    "            tn +=1\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "            \n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \n",
    "    fp = 0\n",
    "    \n",
    "    ### ADD YOUR CODE HERE ###\n",
    "    # Compute the false positives here\n",
    "    actual_y = list(y_true)\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and actual_y[i] == 0:\n",
    "            fp +=1\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \n",
    "    fn = 0\n",
    "    \n",
    "    ### ADD YOUR CODE HERE ###\n",
    "    # Compute the false negatives here\n",
    "    actual_y = list(y_true)\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 0 and actual_y[i] == 1:\n",
    "            fn +=1\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "            \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034227,
     "end_time": "2021-03-22T16:59:28.832850",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.798623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Approch To compute precision of multi class classification problem**\n",
    "\n",
    "There are two different ways to calculate this which might get confusing from time to time. We know that precision depends on true positives and false positives.\n",
    "\n",
    "- **Macro averaged precision**: calculate precision for all classes individually and then average them\n",
    "- **Micro averaged precision**: calculate class wise true positive and false positive and then use that to calculate overall precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034006,
     "end_time": "2021-03-22T16:59:28.901647",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.867641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s see how macro-averaged precision is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:28.980062Z",
     "iopub.status.busy": "2021-03-22T16:59:28.979267Z",
     "iopub.status.idle": "2021-03-22T16:59:28.983041Z",
     "shell.execute_reply": "2021-03-22T16:59:28.982383Z"
    },
    "papermill": {
     "duration": 0.047791,
     "end_time": "2021-03-22T16:59:28.983184",
     "exception": false,
     "start_time": "2021-03-22T16:59:28.935393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Computation of macro-averaged precision\n",
    "\n",
    "def macro_precision(y_true, y_pred):\n",
    "\n",
    "    # find the number of unique classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    classes = np.unique(y_true)\n",
    "    # loop over all classes individually and then average them\n",
    "    ### ADD YOUR CODE HERE ###\n",
    "    for item in classes:\n",
    "        normalized_y_true = [1 if i == item else 0 for i in y_true]\n",
    "        normalized_predictions = [1 if i == item else 0 for i in y_pred]\n",
    "        tp = true_positive(normalized_y_true, normalized_predictions)\n",
    "        fp = false_positive(normalized_y_true, normalized_predictions)\n",
    "        if (tp+fp) != 0:\n",
    "            precision += (tp)/(tp+fp)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    \n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:29.063817Z",
     "iopub.status.busy": "2021-03-22T16:59:29.062930Z",
     "iopub.status.idle": "2021-03-22T16:59:29.067987Z",
     "shell.execute_reply": "2021-03-22T16:59:29.066886Z"
    },
    "papermill": {
     "duration": 0.050873,
     "end_time": "2021-03-22T16:59:29.068201",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.017328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged Precision score : 0.4888881482222149\n"
     ]
    }
   ],
   "source": [
    "print(f\"Macro-averaged Precision score : {macro_precision(y_test, y_pred) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035301,
     "end_time": "2021-03-22T16:59:29.138769",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.103468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see how we can implement marco-averaged precision using sklearn\n",
    "\n",
    "**[Scikit-learn user guide for Precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:29.214182Z",
     "iopub.status.busy": "2021-03-22T16:59:29.213439Z",
     "iopub.status.idle": "2021-03-22T16:59:29.222550Z",
     "shell.execute_reply": "2021-03-22T16:59:29.221892Z"
    },
    "papermill": {
     "duration": 0.049512,
     "end_time": "2021-03-22T16:59:29.222717",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.173205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "macro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:29.301268Z",
     "iopub.status.busy": "2021-03-22T16:59:29.300563Z",
     "iopub.status.idle": "2021-03-22T16:59:29.304185Z",
     "shell.execute_reply": "2021-03-22T16:59:29.303442Z"
    },
    "papermill": {
     "duration": 0.046125,
     "end_time": "2021-03-22T16:59:29.304375",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.258250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-Averaged Precision score using sklearn library : 0.4888881482222149\n"
     ]
    }
   ],
   "source": [
    "print(f\"Macro-Averaged Precision score using sklearn library : {macro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03509,
     "end_time": "2021-03-22T16:59:29.376043",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.340953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s see how micro-averaged precision is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:29.450194Z",
     "iopub.status.busy": "2021-03-22T16:59:29.449435Z",
     "iopub.status.idle": "2021-03-22T16:59:29.455858Z",
     "shell.execute_reply": "2021-03-22T16:59:29.456326Z"
    },
    "papermill": {
     "duration": 0.045528,
     "end_time": "2021-03-22T16:59:29.456605",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.411077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def micro_precision(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    # loop over all classes\n",
    "    ### ADD YOUR CODE HERE ###\n",
    "    # calculate class wise true positive and false positive and then use that to calculate overall precision\n",
    "    for item in np.unique(y_pred):\n",
    "        normalized_y_true = [1 if i == item else 0 for i in y_true]\n",
    "        normalized_predictions = [1 if i == item else 0 for i in y_pred]\n",
    "        tp += true_positive(normalized_y_true, normalized_predictions)\n",
    "        fp += false_positive(normalized_y_true, normalized_predictions)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:29.535940Z",
     "iopub.status.busy": "2021-03-22T16:59:29.535072Z",
     "iopub.status.idle": "2021-03-22T16:59:29.538301Z",
     "shell.execute_reply": "2021-03-22T16:59:29.538801Z"
    },
    "papermill": {
     "duration": 0.046717,
     "end_time": "2021-03-22T16:59:29.538970",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.492253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged Precision score : 0.65625\n"
     ]
    }
   ],
   "source": [
    "print(f\"Micro-averaged Precision score : {micro_precision(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036495,
     "end_time": "2021-03-22T16:59:29.611584",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.575089",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see how we can implement mirco-averaged precision using sklearn\n",
    "\n",
    "**[Scikit-learn user guide for Precision ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:29.689360Z",
     "iopub.status.busy": "2021-03-22T16:59:29.688700Z",
     "iopub.status.idle": "2021-03-22T16:59:29.695400Z",
     "shell.execute_reply": "2021-03-22T16:59:29.695889Z"
    },
    "papermill": {
     "duration": 0.048406,
     "end_time": "2021-03-22T16:59:29.696068",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.647662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-Averaged Precision score using sklearn library : 0.65625\n"
     ]
    }
   ],
   "source": [
    "micro_averaged_precision = metrics.precision_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged Precision score using sklearn library : {micro_averaged_precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03625,
     "end_time": "2021-03-22T16:59:29.770837",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.734587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Recall</h1><a id = \"4\" ></a>\n",
    "\n",
    "\n",
    "- **[Recall(Sensitivity)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived\n",
    "\n",
    "<h2 style = \"text-align: center\"> Recall = TP / (TP + FN) </h2>\n",
    "\n",
    "\n",
    "\n",
    "**Approch To compute recall of multi class classification problem**\n",
    "\n",
    "There are two different ways to calculate this which might get confusing from time to time. We know that recall depends on true positives and false negatives.\n",
    "\n",
    "- **Macro averaged recall**: calculate recall for all classes individually and then average them\n",
    "- **Micro averaged recall**: calculate class wise true positive and false negative and then use that to calculate overall recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0362,
     "end_time": "2021-03-22T16:59:29.843265",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.807065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s see how macro-averaged recall is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:29.923686Z",
     "iopub.status.busy": "2021-03-22T16:59:29.923003Z",
     "iopub.status.idle": "2021-03-22T16:59:29.925364Z",
     "shell.execute_reply": "2021-03-22T16:59:29.925887Z"
    },
    "papermill": {
     "duration": 0.047015,
     "end_time": "2021-03-22T16:59:29.926062",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.879047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Computation of macro-averaged recall\n",
    "\n",
    "def macro_recall(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize recall to 0\n",
    "    recall = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        normalized_y_true = [1 if i == class_ else 0 for i in y_true]\n",
    "        normalized_predictions = [1 if i == class_ else 0 for i in y_pred]\n",
    "        fn = false_negative(normalized_y_true, normalized_predictions)\n",
    "        tp = true_positive(normalized_y_true, normalized_predictions)\n",
    "        if (fn +tp) != 0:\n",
    "            recall += tp/(fn+tp)\n",
    "        \n",
    "    ### ADD YOUR CODE HERE ###\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # calculate and return average recall over all classes\n",
    "    recall /= num_classes\n",
    "    \n",
    "    return recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:30.007645Z",
     "iopub.status.busy": "2021-03-22T16:59:30.006826Z",
     "iopub.status.idle": "2021-03-22T16:59:30.010800Z",
     "shell.execute_reply": "2021-03-22T16:59:30.010150Z"
    },
    "papermill": {
     "duration": 0.047304,
     "end_time": "2021-03-22T16:59:30.010951",
     "exception": false,
     "start_time": "2021-03-22T16:59:29.963647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged recall score : 0.35440616320834545\n"
     ]
    }
   ],
   "source": [
    "print(f\"Macro-averaged recall score : {macro_recall(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036375,
     "end_time": "2021-03-22T16:59:30.085259",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.048884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see how we can implement macro-averaged recall using sklearn\n",
    "\n",
    " **[Scikit-learn user guide for Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:30.167568Z",
     "iopub.status.busy": "2021-03-22T16:59:30.165897Z",
     "iopub.status.idle": "2021-03-22T16:59:30.172123Z",
     "shell.execute_reply": "2021-03-22T16:59:30.173149Z"
    },
    "papermill": {
     "duration": 0.050381,
     "end_time": "2021-03-22T16:59:30.173417",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.123036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged recall score using sklearn : 0.35440616320834545\n"
     ]
    }
   ],
   "source": [
    "macro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-averaged recall score using sklearn : {macro_averaged_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03696,
     "end_time": "2021-03-22T16:59:30.248474",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.211514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s see how micro-averaged recall is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:30.330914Z",
     "iopub.status.busy": "2021-03-22T16:59:30.330149Z",
     "iopub.status.idle": "2021-03-22T16:59:30.333108Z",
     "shell.execute_reply": "2021-03-22T16:59:30.333717Z"
    },
    "papermill": {
     "duration": 0.048198,
     "end_time": "2021-03-22T16:59:30.333929",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.285731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def micro_recall(y_true, y_pred):\n",
    "\n",
    "\n",
    "    # find the number of classes \n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in y_true.unique():\n",
    "         ### ADD YOUR CODE HERE ###\n",
    "        normalized_y_true = [1 if i == class_ else 0 for i in y_true]\n",
    "        normalized_predictions = [1 if i == class_ else 0 for i in y_pred]\n",
    "        fn += false_negative(normalized_y_true, normalized_predictions)\n",
    "        tp += true_positive(normalized_y_true, normalized_predictions)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    # calculate and return overall recall\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:30.416979Z",
     "iopub.status.busy": "2021-03-22T16:59:30.416304Z",
     "iopub.status.idle": "2021-03-22T16:59:30.420224Z",
     "shell.execute_reply": "2021-03-22T16:59:30.419620Z"
    },
    "papermill": {
     "duration": 0.048486,
     "end_time": "2021-03-22T16:59:30.420371",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.371885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged recall score : 0.65625\n"
     ]
    }
   ],
   "source": [
    "print(f\"Micro-averaged recall score : {micro_recall(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038074,
     "end_time": "2021-03-22T16:59:30.496468",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.458394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see how we can implement micro-averaged recall using sklearn\n",
    "\n",
    " **[Scikit-learn user guide for Recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:30.580442Z",
     "iopub.status.busy": "2021-03-22T16:59:30.579781Z",
     "iopub.status.idle": "2021-03-22T16:59:30.585609Z",
     "shell.execute_reply": "2021-03-22T16:59:30.585002Z"
    },
    "papermill": {
     "duration": 0.049456,
     "end_time": "2021-03-22T16:59:30.585759",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.536303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-Averaged recall score using sklearn library : 0.65625\n"
     ]
    }
   ],
   "source": [
    "micro_averaged_recall = metrics.recall_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged recall score using sklearn library : {micro_averaged_recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038469,
     "end_time": "2021-03-22T16:59:30.663247",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.624778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">F1 score</h1><a id = \"5\" ></a>\n",
    "\n",
    "\n",
    "- **[F1 score (F1)](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)** : F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n",
    "\n",
    "F1 score is a metric that combines both precision and recall. It is defined as a simple weighted average (harmonic mean) of precision and recall. If we denote precision using P and recall using R, we can represent the F1 score as:\n",
    "\n",
    "<h2 style = \"text-align: center\"> F1 = 2PR / (P + R) </h2>\n",
    "\n",
    "\n",
    "\n",
    "**Approch To compute F1 Score of multi class classification problem**\n",
    "\n",
    "There are two different ways to calculate this which might get confusing from time to time. We know that F1 Score depends on precision and recall.\n",
    "\n",
    "- **Macro averaged F1 Score**: calculate f1 score of every class and then average them\n",
    "- **Micro averaged F1 Score**: calculate macro-averaged precision score and macro-averaged recall score and then take there harmonic mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038409,
     "end_time": "2021-03-22T16:59:30.740092",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.701683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s see how macro-averaged f1 score is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:30.827218Z",
     "iopub.status.busy": "2021-03-22T16:59:30.826113Z",
     "iopub.status.idle": "2021-03-22T16:59:30.829645Z",
     "shell.execute_reply": "2021-03-22T16:59:30.829116Z"
    },
    "papermill": {
     "duration": 0.051065,
     "end_time": "2021-03-22T16:59:30.829794",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.778729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Computation of macro-averaged fi score\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "\n",
    "    # find the number of classes\n",
    "    num_classes = len(np.unique(y_true))\n",
    "\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in list(y_true.unique()):\n",
    "        \n",
    "        # all classes except current are considered negative\n",
    "        \n",
    "         ### ADD YOUR CODE HERE ###\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        fn = false_negative(temp_true, temp_pred)\n",
    "        precision = 0\n",
    "        if (tp+fp) != 0:\n",
    "            precision  = (tp)/(tp+fn)\n",
    "        recall = 0;\n",
    "        if (tp+fn) != 0:\n",
    "            recall = tp/(tp+fn)\n",
    "        temp_f1=0\n",
    "        if precision+recall != 0:\n",
    "            temp_f1 = (2*precision*recall)/(precision+recall)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # keep adding f1 score for all classes\n",
    "        f1 += temp_f1\n",
    "        \n",
    "    # calculate and return average f1 score over all classes\n",
    "    f1 /= num_classes\n",
    "    \n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:30.914370Z",
     "iopub.status.busy": "2021-03-22T16:59:30.913558Z",
     "iopub.status.idle": "2021-03-22T16:59:30.916967Z",
     "shell.execute_reply": "2021-03-22T16:59:30.916377Z"
    },
    "papermill": {
     "duration": 0.048955,
     "end_time": "2021-03-22T16:59:30.917117",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.868162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged f1 score : 0.4676465813268123\n"
     ]
    }
   ],
   "source": [
    "print(f\"Macro-averaged f1 score : {macro_f1(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0398,
     "end_time": "2021-03-22T16:59:30.996781",
     "exception": false,
     "start_time": "2021-03-22T16:59:30.956981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see how we can implement micro-averaged F1 score using sklearn\n",
    "\n",
    "**[Scikit-learn user guide for F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:31.082955Z",
     "iopub.status.busy": "2021-03-22T16:59:31.082142Z",
     "iopub.status.idle": "2021-03-22T16:59:31.087582Z",
     "shell.execute_reply": "2021-03-22T16:59:31.088089Z"
    },
    "papermill": {
     "duration": 0.051457,
     "end_time": "2021-03-22T16:59:31.088260",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.036803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-Averaged F1 score using sklearn library : 0.366617620831104\n"
     ]
    }
   ],
   "source": [
    "macro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "print(f\"Macro-Averaged F1 score using sklearn library : {macro_averaged_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.040399,
     "end_time": "2021-03-22T16:59:31.168150",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.127751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s see how micro-averaged f1 score is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:31.253845Z",
     "iopub.status.busy": "2021-03-22T16:59:31.253102Z",
     "iopub.status.idle": "2021-03-22T16:59:31.257000Z",
     "shell.execute_reply": "2021-03-22T16:59:31.256367Z"
    },
    "papermill": {
     "duration": 0.049196,
     "end_time": "2021-03-22T16:59:31.257138",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.207942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def micro_f1(y_true, y_pred):\n",
    "\n",
    "\n",
    "    #micro-averaged precision score\n",
    "    P = micro_precision(y_true, y_pred)\n",
    "\n",
    "    #micro-averaged recall score\n",
    "    R = micro_recall(y_true, y_pred)\n",
    "\n",
    "    #micro averaged f1 score\n",
    "    f1 = 2*P*R / (P + R)    \n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:31.347261Z",
     "iopub.status.busy": "2021-03-22T16:59:31.346285Z",
     "iopub.status.idle": "2021-03-22T16:59:31.350265Z",
     "shell.execute_reply": "2021-03-22T16:59:31.349615Z"
    },
    "papermill": {
     "duration": 0.053405,
     "end_time": "2021-03-22T16:59:31.350408",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.297003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Micro-averaged recall score : {micro_f1(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039605,
     "end_time": "2021-03-22T16:59:31.430176",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.390571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see how we can implement micro-averaged F1 score using sklearn\n",
    "\n",
    "**[Scikit-learn user guide for F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:31.516470Z",
     "iopub.status.busy": "2021-03-22T16:59:31.515473Z",
     "iopub.status.idle": "2021-03-22T16:59:31.521545Z",
     "shell.execute_reply": "2021-03-22T16:59:31.520909Z"
    },
    "papermill": {
     "duration": 0.05138,
     "end_time": "2021-03-22T16:59:31.521690",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.470310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "micro_averaged_f1 = metrics.f1_score(y_test, y_pred, average = 'micro')\n",
    "print(f\"Micro-Averaged F1 score using sklearn library : {micro_averaged_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.040223,
     "end_time": "2021-03-22T16:59:31.602602",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.562379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039952,
     "end_time": "2021-03-22T16:59:31.683281",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.643329",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Area under the ROC curve</h1><a id = \"6\" ></a>\n",
    "\n",
    "\n",
    "- **[Area under the ROC (Receiver Operating Characteristic) curve](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)** : AUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0s as 0s and 1s as 1s. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.\n",
    "\n",
    "     **[Scikit-learn user guide for AUC under the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)**\n",
    "     \n",
    " ![](https://glassboxmedicine.files.wordpress.com/2019/02/roc-curve-v2.png?w=576)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041133,
     "end_time": "2021-03-22T16:59:31.765054",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.723921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Approch to compute AUC score of multi class classification problem**\n",
    "\n",
    "**One vs All** : It involves splitting the multi-class dataset into multiple binary classification problems. A binary classifier is then trained on each binary classification problem and predictions are made using the model that is the most confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:31.855317Z",
     "iopub.status.busy": "2021-03-22T16:59:31.854593Z",
     "iopub.status.idle": "2021-03-22T16:59:31.858062Z",
     "shell.execute_reply": "2021-03-22T16:59:31.857395Z"
    },
    "papermill": {
     "duration": 0.052017,
     "end_time": "2021-03-22T16:59:31.858223",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.806206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average = \"macro\"):\n",
    "    \n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(actual_class)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        \n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average)\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "    return roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:31.947517Z",
     "iopub.status.busy": "2021-03-22T16:59:31.946875Z",
     "iopub.status.idle": "2021-03-22T16:59:31.964230Z",
     "shell.execute_reply": "2021-03-22T16:59:31.963725Z"
    },
    "papermill": {
     "duration": 0.064549,
     "end_time": "2021-03-22T16:59:31.964444",
     "exception": false,
     "start_time": "2021-03-22T16:59:31.899895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "roc_auc_dict = roc_auc_score_multiclass(y_test, y_pred)\n",
    "roc_auc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041863,
     "end_time": "2021-03-22T16:59:32.047336",
     "exception": false,
     "start_time": "2021-03-22T16:59:32.005473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Confusion Matrix</h1><a id = \"7\" ></a>\n",
    "\n",
    "- **[Confusion Matrix](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/)** : A much better way to evaluate the performance of a classifier is to look at the confusion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.\n",
    "\n",
    "![](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)     \n",
    "\n",
    "\n",
    "    **[Scikit-Learn user guide for Confusion Matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:32.136096Z",
     "iopub.status.busy": "2021-03-22T16:59:32.135390Z",
     "iopub.status.idle": "2021-03-22T16:59:32.138108Z",
     "shell.execute_reply": "2021-03-22T16:59:32.138710Z"
    },
    "papermill": {
     "duration": 0.050235,
     "end_time": "2021-03-22T16:59:32.138890",
     "exception": false,
     "start_time": "2021-03-22T16:59:32.088655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-22T16:59:32.226431Z",
     "iopub.status.busy": "2021-03-22T16:59:32.225776Z",
     "iopub.status.idle": "2021-03-22T16:59:32.659225Z",
     "shell.execute_reply": "2021-03-22T16:59:32.659883Z"
    },
    "papermill": {
     "duration": 0.478313,
     "end_time": "2021-03-22T16:59:32.660191",
     "exception": false,
     "start_time": "2021-03-22T16:59:32.181878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,8))\n",
    "sns.heatmap(metrics.confusion_matrix(y_test, y_pred), annot = True, xticklabels = y_test.unique(), yticklabels = y_test.unique(), cmap = 'summer')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.042505,
     "end_time": "2021-03-22T16:59:32.745577",
     "exception": false,
     "start_time": "2021-03-22T16:59:32.703072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.315211,
   "end_time": "2021-03-22T16:59:33.498906",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-22T16:59:19.183695",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
